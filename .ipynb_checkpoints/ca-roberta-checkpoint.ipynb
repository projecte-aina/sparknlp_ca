{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PElFTvY1JcH",
    "outputId": "8f0c0e10-5afc-46a9-c966-f3be77b26f13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-01 15:15:25--  http://setup.johnsnowlabs.com/colab.sh\n",
      "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
      "--2022-06-01 15:15:25--  https://setup.johnsnowlabs.com/colab.sh\n",
      "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
      "--2022-06-01 15:15:25--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1603 (1.6K) [text/plain]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                     0%[                    ]       0  --.-KB/s               setup Colab for PySpark 3.0.3 and Spark NLP 3.4.4\n",
      "Installing PySpark 3.0.3 and Spark NLP 3.4.4\n",
      "-                   100%[===================>]   1.57K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2022-06-01 15:15:25 (2.04 MB/s) - written to stdout [1603/1603]\n",
      "\n",
      "\u001b[K     |████████████████████████████████| 209.1 MB 63 kB/s \n",
      "\u001b[K     |████████████████████████████████| 145 kB 50.9 MB/s \n",
      "\u001b[K     |████████████████████████████████| 198 kB 60.5 MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# This is only to setup PySpark and Spark NLP on Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x0taftjJ5ThL",
    "outputId": "16991bfc-b2b1-4847-ee29-93208e4f2622"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-01 15:41:01--  https://github.com/projecte-aina/sparknlp_ca/releases/download/NER_v2/roberta-base-ca-cased-ner_spark_nlp.zip\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/495290333/729c19f5-8183-43d4-a730-3a770cd4b7e7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220601%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220601T154101Z&X-Amz-Expires=300&X-Amz-Signature=bb60beb6d63e355e8ed6a4e305e6dc29b90956621a6fdd3251d2e11e93f8a091&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=495290333&response-content-disposition=attachment%3B%20filename%3Droberta-base-ca-cased-ner_spark_nlp.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2022-06-01 15:41:01--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/495290333/729c19f5-8183-43d4-a730-3a770cd4b7e7?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220601%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220601T154101Z&X-Amz-Expires=300&X-Amz-Signature=bb60beb6d63e355e8ed6a4e305e6dc29b90956621a6fdd3251d2e11e93f8a091&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=495290333&response-content-disposition=attachment%3B%20filename%3Droberta-base-ca-cased-ner_spark_nlp.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 442775791 (422M) [application/octet-stream]\n",
      "Saving to: ‘roberta-base-ca-cased-ner_spark_nlp.zip’\n",
      "\n",
      "roberta-base-ca-cas 100%[===================>] 422.26M   129MB/s    in 3.3s    \n",
      "\n",
      "2022-06-01 15:41:05 (130 MB/s) - ‘roberta-base-ca-cased-ner_spark_nlp.zip’ saved [442775791/442775791]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/projecte-aina/sparknlp_ca/releases/download/NER_v2/roberta-base-ca-cased-ner_spark_nlp.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPOL7v_a7X6J",
    "outputId": "67d02f68-dfcd-48d9-aa87-b9f1ed896725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  roberta-base-ca-cased-ner_spark_nlp.zip\n",
      "   creating: roberta-base-ca-cased-ner_spark_nlp/\n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/.roberta_classification_tensorflow.crc  \n",
      "   creating: roberta-base-ca-cased-ner_spark_nlp/fields/\n",
      "   creating: roberta-base-ca-cased-ner_spark_nlp/fields/labels/\n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/labels/._SUCCESS.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/labels/.part-00000.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/labels/.part-00001.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/labels/.part-00002.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/labels/.part-00003.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/labels/_SUCCESS  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/labels/part-00000  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/labels/part-00001  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/labels/part-00002  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/labels/part-00003  \n",
      "   creating: roberta-base-ca-cased-ner_spark_nlp/fields/merges/\n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/merges/._SUCCESS.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/merges/.part-00000.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/merges/.part-00001.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/merges/.part-00002.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/merges/.part-00003.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/merges/_SUCCESS  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/merges/part-00000  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/merges/part-00001  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/merges/part-00002  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/merges/part-00003  \n",
      "   creating: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/\n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/._SUCCESS.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/.part-00000.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/.part-00001.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/.part-00002.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/.part-00003.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/_SUCCESS  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/part-00000  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/part-00001  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/part-00002  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/signatures/part-00003  \n",
      "   creating: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/\n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/._SUCCESS.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/.part-00000.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/.part-00001.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/.part-00002.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/.part-00003.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/_SUCCESS  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/part-00000  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/part-00001  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/part-00002  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/fields/vocabulary/part-00003  \n",
      "   creating: roberta-base-ca-cased-ner_spark_nlp/metadata/\n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/metadata/._SUCCESS.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/metadata/.part-00000.crc  \n",
      " extracting: roberta-base-ca-cased-ner_spark_nlp/metadata/_SUCCESS  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/metadata/part-00000  \n",
      "  inflating: roberta-base-ca-cased-ner_spark_nlp/roberta_classification_tensorflow  \n"
     ]
    }
   ],
   "source": [
    "!unzip roberta-base-ca-cased-ner_spark_nlp.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZOjOBWk1qUO",
    "outputId": "4ee642ba-1182-47cd-ab34-9f82236c8484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 3.4.4\n",
      "Apache Spark version: 3.0.3\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()\n",
    "# params =>> gpu=False, spark23=False (start with spark 2.3)\n",
    "\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "cMC15MsK1zj1",
    "outputId": "77a2a69c-fa7c-4fea-f1c9-653e31be3eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version:  3.4.4\n",
      "Apache Spark version:  3.0.3\n",
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 514.9 KB\n",
      "[OK!]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e41600bd0262>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0msentencerDL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceDetectorDLModel\u001b[0m  \u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence_detector_dl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"xx\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"document\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0msetPrefixes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"’\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'”'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"(\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"l'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"l’\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"s'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"s’\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"d’\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"d'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"m’\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"m'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"L'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"L’\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"S’\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"S'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"N’\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"N'\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"M’\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"M'\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msetWhitelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aprox.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"pàg.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"p.ex.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gen.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"feb.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"abr.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"jul.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"set.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"oct.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"nov.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dec.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Dr.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Dra.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sr.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sra.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Srta.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"núm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"St.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Sta.\"\u001b[0m\u001b[0;34m...\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRoBertaForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"roberta-base-ca-cased-ner_spark_nlp\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0msetInputCols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"token\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0msetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tokenizer' object has no attribute 'setPrefixes'"
     ]
    }
   ],
   "source": [
    "\n",
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start()#spark32=True)\n",
    "\n",
    "print(\"Spark NLP version: \", sparknlp.version())\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "\n",
    "#Spark NLP version:  3.4.4\n",
    "#Apache Spark version:  3.1.2\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"text\")\\\n",
    "      .setOutputCol(\"document\")\\\n",
    "      .setCleanupMode(\"shrink_full\")\n",
    "\n",
    "sentencerDL = SentenceDetectorDLModel\\\n",
    "  .pretrained(\"sentence_detector_dl\", \"xx\") \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['sentence']) \\\n",
    "    .setOutputCol('token')\\\n",
    "    .setPrefixes([\"’\", '”', \"(\", \"[\", \"l'\",\"l’\",\"s'\",\"s’\",\"d’\",\"d'\",\"m’\",\"m'\",\"L'\",\"L’\",\"S’\",\"S'\",\"N’\",\"N'\",\"M’\",\"M'\"]) \\\n",
    "    .setWhitelist([\"aprox.\",\"pàg.\",\"p.ex.\",\"gen.\",\"feb.\",\"abr.\",\"jul.\",\"set.\",\"oct.\",\"nov.\",\"dec.\",\"Dr.\",\"Dra.\",\"Sr.\",\"Sra.\",\"Srta.\",\"núm\",\"St.\",\"Sta.\",\"pl.\",\"etc.\"] ) \\\n",
    "    .setSuffixes([\"-ho\",\"'ls\",\"'l\",\"'ns\",\"'t\",\"'m\",\"'n\",\"’ls\",\"’l\",\"’ns\",\"’t\",\"’m\",\"’n\",\"-les\",\"-la\",\"-lo\",\"-li\",\"-los\",\"-me\",\"-nos\",\"-te\",\"-vos\",\"-se\",\"-hi\",\"-ne\",\"-en\",'.', ':', '%', ',', ';', '?', \"'\", '\"', ')', ']', '!'])\n",
    "\n",
    "ner = RoBertaForTokenClassification.load(\"roberta-base-ca-cased-ner_spark_nlp\")\\\n",
    "    .setInputCols(\"token\", \"sentence\")\\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "nerconverter = NerConverter()\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "    .setOutputCol(\"entities\")\\\n",
    "    #.setWhiteList(['ORG','LOC','PER','MISC'])\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "    documentAssembler, \n",
    "    sentencerDL,\n",
    "    tokenizer,\n",
    "    ner,\n",
    "    nerconverter\n",
    " ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hEdLhsVv47iM"
   },
   "outputs": [],
   "source": [
    "#Aplicacion de los pipelines y visualización de los resultados\n",
    "text = \"Veig a l'home dels Estats Units amb el telescopi.\"\n",
    "\n",
    "spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "pipelineModel = nlpPipeline.fit(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jTKO3JYi5FJR"
   },
   "outputs": [],
   "source": [
    "result = pipelineModel.transform(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IVpn1Yt8dsK",
    "outputId": "7d41bd8b-3ef6-4080-f367-41a62b60cbd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|result                                |\n",
      "+--------------------------------------+\n",
      "|[O, O, O, O, B-LOC, I-LOC, O, O, O, O]|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('ner.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iv-d1euG7kSR",
    "outputId": "b94905ee-1474-4aa1-f1ea-70f710cd8ebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|                 ner|            entities|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Veig a l'home del...|[[document, 0, 48...|[[document, 0, 48...|[[token, 0, 3, Ve...|[[named_entity, 0...|[[chunk, 19, 30, ...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04DBPt_4_3MF",
    "outputId": "13b11575-95e8-4c56-ae07-b2eca4702d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|result        |\n",
      "+--------------+\n",
      "|[Estats Units]|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select('entities.result').show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
